import os
import torch
import json
from contextlib import contextmanager
import numpy as np
from medusa.model.medusa_model import MedusaModel
from medusa.model.kv_cache import *
from medusa.model.utils import *
from medusa.model.medusa_choices import *
from copy import deepcopy
import matplotlib.pyplot as plt
import torch.nn.functional as F
from fastchat.model.model_adapter import get_conversation_template
from tqdm import tqdm
import argparse
import time

def get_accuracy(sv_logits, target_id):
    sv_logits = sv_logits.view(-1, sv_logits.shape[-1])
    target_id = target_id.view(-1)
    # sv_logits: (samples, vocab_size)
    # target_id: (samples)
    samples = sv_logits.shape[0]
    vocab_size = sv_logits.shape[-1]
    accuracy = torch.zeros((vocab_size, samples))
    # for each vocab_size is the topk accuracy
    # for every element in the last dimension, is boolean, denoting whether the target_id is in the topk
    for k in range(vocab_size):
        topk = torch.topk(sv_logits, k + 1, dim=-1).indices
        accuracy[k] = torch.any(topk == target_id.unsqueeze(1), dim=-1)
    return accuracy


def main(args):
    model = MedusaModel.from_pretrained(
        args.model_path,
        # medusa_num_heads=args.medusa_num_heads,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
        device_map="auto"
    )
    tokenizer = model.get_tokenizer()


    data = json.load(open(args.data_path))
    past_key_values, past_key_values_data, current_length_data = initialize_past_key_values(model.base_model)
    model.past_key_values = past_key_values
    model.past_key_values_data = past_key_values_data
    model.current_length_data = current_length_data
    results = None
    accuracy = []
    steps = args.steps

    expected = []
    actual_logits = []


    for sample in tqdm(data):
        conv = get_conversation_template("vicuna")
        conv.messages = []
        conv.append_message(conv.roles[0], sample["instruction"])
        conv.append_message(conv.roles[1], "")
        prompt = conv.get_prompt()

        with torch.inference_mode():
            embedding = model.base_model.get_input_embeddings().weight
            # collect all logits generated by the target_model
            input_ids = tokenizer([prompt]).input_ids
            prompt_len = len(input_ids[0])
            input_ids = torch.as_tensor(data=input_ids).cuda()
            sv_embeds_list = []

            # prefill
            model.current_length_data.zero_() # this is for rerun
            reset_medusa_mode(model)

            medusa_logits, outputs, logits = model(
                input_ids=input_ids, past_key_values=past_key_values, output_orig=True, medusa_forward=True
            )
            medusa_logits = medusa_logits[...,-1,:] # (num_heads, bs, vocab_size)
            logits = logits[:, -1,:].unsqueeze(0) # (1, bs, vocab_size)
            sv_logits = torch.cat([logits, medusa_logits], dim=0)
            input_id = logits[:, -1:, :].argmax(dim=-1)
            input_ids = torch.cat([input_ids, input_id], dim=-1)
            sv_embeds = (F.softmax(sv_logits, dim=-1) @ embedding).transpose(0, 1)
            sv_embeds_list.append(sv_embeds)

            for _ in range(steps):
                # use base_model to generate logits and decode the next token
                medusa_logits, outputs, logits = model(
                    input_ids=input_id, past_key_values=past_key_values, output_orig=True, medusa_forward=True
                )
                medusa_logits = medusa_logits[...,-1,:]
                logits = logits[:,-1,:].unsqueeze(0)
                sv_logits = torch.cat([logits, medusa_logits], dim=0)
                sv_embeds = (F.softmax(sv_logits, dim=-1) @ embedding).transpose(0, 1)
                sv_embeds_list.append(sv_embeds)
                input_id = logits[:, -1:, :].argmax(dim=-1)
                input_ids = torch.cat([input_ids, input_id], dim=-1)

            precessed_inputs_embeds = model.base_model.get_input_embeddings()(input_ids)
            model.current_length_data.zero_() # this is for rerun
            reset_medusa_mode(model)


            # prefill for sv decoding
            prev_input_len = prompt_len
            inputs_embeds = precessed_inputs_embeds[:, :prev_input_len, :]
            sv_embeds = sv_embeds_list[0]
            medusa_logits, outputs, logits = model(
                inputs_embeds=torch.cat([inputs_embeds, sv_embeds], dim=1), past_key_values=past_key_values, output_orig=True, medusa_forward=True
            )
            # remove the kv-cache for sv_embeds
            model.current_length_data.fill_(prev_input_len)
            logits = logits[:, -1, :]
            target_id = input_ids[:, prev_input_len + sv_embeds.shape[1]]
            actual_logits.append(logits)
            expected.append(target_id)

            for i in range(steps):
                prev_input_len = prompt_len + 1 + i
                sv_embeds = sv_embeds_list[i]
                if prev_input_len + sv_embeds.shape[1] + 1 >= input_ids.shape[1]:
                    break
                medusa_logits, outputs, logits = model(
                    inputs_embeds=sv_embeds, past_key_values=past_key_values, output_orig=True, medusa_forward=True
                )
                # remove the kv-cache for sv_embeds
                model.current_length_data.fill_(prev_input_len)
                logits = logits[:, -1, :]
                target_id = input_ids[:, prev_input_len + sv_embeds.shape[1]]
                actual_logits.append(logits)
                expected.append(target_id)
    
    expected = torch.stack(expected, dim=0).transpose(0, 1)
    actual_logits = torch.stack(actual_logits, dim=0).transpose(0, 1)
    accuracy = get_accuracy(actual_logits, expected)
    save_path = os.path.join(args.save_dir, args.model_name + "_sv_accuracy.pt")
    torch.save(accuracy, save_path)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Medusa Model Evaluator")

    parser.add_argument("--model_path", type=str, required=True,
                        help="Path to the pre-trained Medusa model.")
    parser.add_argument("--model_name", type=str, required=True,
                        help="Name of the model.")
    parser.add_argument("--medusa_num_heads", type=int, default=5,
                        help="Number of medusa heads.")
    parser.add_argument("--data_path", type=str, required=True,
                        help="Path to the evaluation data in JSON format.")
    parser.add_argument("--save_dir", type=str, default="../../data",
                        help="Directory to save the results.")
    parser.add_argument("--steps", type=int, default=20,
                        help="Number of steps to run the model.")
    args = parser.parse_args()

    # If the save directory doesn't exist, create it
    if not os.path.exists(args.save_dir):
        os.makedirs(args.save_dir)
    main(args)